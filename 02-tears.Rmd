# Data

## Data Source

The data used for this project is based on different Wikipedia Articles. In detail following url's:

1) https://de.wikipedia.org/wiki/Liste_ehemaliger_NSDAP-Mitglieder,_die_nach_Mai_1945_politisch_t%C3%A4tig_waren

2) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(1._Wahlperiode)

3) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(2._Wahlperiode)

4) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(3._Wahlperiode)

5) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(4._Wahlperiode)

6) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(5._Wahlperiode)

7) https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(6._Wahlperiode)

## Data Scraping and Wrangling

### Bundestag Member

This code is a script for web scraping and data wrangling in R. It collects data on members of the German Bundestag from different election periods and cleans the data into a tidy format. The code uses the "tidyverse" and "rvest" packages to perform the web scraping and data manipulation.

The script starts by scraping data from Wikipedia for the first election period, the first Bundestag (1.WP). It starts by setting the URL for the relevant Wikipedia page and reading the HTML content of the page. Then, it selects the desired table from the HTML content and extracts the relevant information from the table, including the links for the names of the members in the first column. The extracted data is then cleaned and transformed into a tidy format, with columns for the member's name, birth/death information, party affiliation, state, constituency, vote share, comments, and the election period.

The script repeats this process for the second, third, and fourth, fifth and sixth election periods, with each iteration scraping data from a different Wikipedia page and cleaning the data into a tidy format with the same columns.

```{r, results='hide', error=FALSE, warning=FALSE, message=FALSE}
# packages ---------------------------------------------------------------------
library(tidyverse)
library(rvest)


################################## 1.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(1._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(5)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)


# 2) data wrangling ------------------------------------------------------------
firstbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebens-daten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Stimmen in %`,
comment = Bemerkungen,
wikitag = links)
firstbt <- firstbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 1)


################################## 2.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(2._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(3)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)

# 2) data wrangling ------------------------------------------------------------
secondbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebens-daten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Erst-stimmen in %`,
comment = Bemerkungen,
wikitag = links)
secondbt <- secondbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 2)


################################## 3.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(3._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(4)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)

# 2) data wrangling ------------------------------------------------------------
thirdbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebensdaten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Erst-stimmen in %`,
comment = Bemerkungen,
wikitag = links)
thirdbt <- thirdbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 3)


################################## 4.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(4._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(2)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)

# 2) data wrangling ------------------------------------------------------------
fourthbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebensdaten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Erst-stimmen in %`,
comment = Bemerkungen,
wikitag = links)
fourthbt <- fourthbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 4)


################################## 5.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(5._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(2)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)

# 2) data wrangling ------------------------------------------------------------
fifthbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebens-daten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Erst-stimmen in %`,
comment = Bemerkungen,
wikitag = links)
fifthbt <- fifthbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 5)

# recode SPD (GDP) and CSU (GDP) as SPD and CSU
# the four members of the GDP party were guests in the fraction of the SPD or CSU
# and therefore coded as members of their fraction

fifthbt <- fifthbt %>%
  mutate(party = recode(party, "SPD (GDP)" = "SPD",
                        "CSU (GDP)" = "CSU"))


################################## 6.WP ########################################

# 1) web scraping --------------------------------------------------------------
url = "https://de.wikipedia.org/wiki/Liste_der_Mitglieder_des_Deutschen_Bundestages_(6._Wahlperiode)"
html = read_html(url)
html %>%
html_nodes(css = "table")
table_node <- html %>%
html_nodes(css = "table") %>%
nth(2)
# Extract the data from the table node
table_data <- table_node %>% html_table(fill = TRUE)
# Extract the links for the names in the first column of the table
table_data$links <- html_attr(table_node %>% html_nodes("td:nth-child(1) a"), "href")
rm(html, table_node, url)

# 2) data wrangling ------------------------------------------------------------
sixthbt <- table_data %>%
rename(name = `Mitglied des Bundestages`,
birth_death = `Lebens-daten`,
party = Partei,
state = `Bundesland/ Landesliste`,
constituency = Wahlkreis,
voteshare = `Erst-stimmen in %`,
comment = Bemerkungen,
wikitag = links)
sixthbt <- sixthbt %>%
select(name, wikitag, birth_death, party,
state, constituency, voteshare, comment) %>%
mutate(legperiod = 6)
```

In the end, all the tidied data is combined into a single data frame, making it easy to analyze and work with the collected information on the members of the German Bundestag across multiple election periods.

```{r, results='hide', error=FALSE, warning=FALSE, message=FALSE}
######################### member complete ######################################
df_list <- list(firstbt, secondbt, thirdbt, fourthbt, fifthbt, sixthbt)
btmember <- do.call(rbind, df_list)

rm(df_list, table_data)

saveRDS(btmember, file = "btmember.RDS")
```

### Former NSDAP members

This code is a web scraper that gathers information about former members of the National Socialist German Workers' Party (NSDAP) who were politically active after May 1945.

The first part of the code uses the rvest library to scrape a table from a Wikipedia page and convert it into a data frame. The second part of the code performs data cleaning and manipulation tasks such as splitting columns, removing unnecessary characters, and replacing empty values with NA. Finally, the columns are renamed and the data is saved as an RDS file for future use.

```{r, results='hide', error=FALSE, warning=FALSE, message=FALSE}
# Packages ---------------------------------------------------------------------

library(tidyverse)
library(rvest)
library(stringr)
library(writexl)

# 1) Web-Scraping --------------------------------------------------------------

# liste brd 
url <- "https://de.wikipedia.org/wiki/Liste_ehemaliger_NSDAP-Mitglieder,_die_nach_Mai_1945_politisch_t%C3%A4tig_waren"

# Read the HTML content of the URL
html_content <- read_html(url)

# Extract the table
table_html <- html_content %>%
  html_nodes("table") %>%
  .[[1]]

# Convert the table to a data frame
nazis.brd <- table_html %>%
  html_table(header = TRUE)

rm(html_content, table_html, url)

# liste brd wikitags
url <- "https://de.wikipedia.org/wiki/Liste_ehemaliger_NSDAP-Mitglieder,_die_nach_Mai_1945_politisch_t%C3%A4tig_waren"

page <- read_html(url)

names_col <- html_nodes(page, ".wikitable td:nth-child(1) a") %>% html_text()
names_col <- gsub("\n", "", names_col)

links_col <- html_nodes(page, ".wikitable td:nth-child(1) a") %>% html_attr("href")

wikitags.brd <- data.frame(name = names_col, links = links_col, stringsAsFactors = FALSE) %>% 
  distinct()

rm(page, links_col, names_col, url)

# 2) Data-Cleaning -------------------------------------------------------------

# left join wikitags
nazis.brd <- nazis.brd %>%
  separate(Name, c("name", "birth_death"), "\\(") %>% 
  distinct() %>% 
  left_join(wikitags.brd, by = "name")

# name, birthdate, deathdate clean
nazis.brd <- nazis.brd %>%
  mutate(name = str_split(name, ",") %>% map_chr(~ paste(rev(.x), collapse = " ")),
         name = str_trim(name),
         birth_death = gsub("[()]", "", birth_death),
         birth_death = str_replace(birth_death, "-", ""),
         birthdate = str_sub(birth_death, 1, 4),
         deathdate = str_sub(birth_death, 6, 9))

nazis.brd["birth_death"] <- NULL

# partymembership clean
nazis.brd <- nazis.brd %>%
  mutate(partybrd = gsub("[0-9]+", "", `Parteimitglied­schaften ab 1945`),
         partybrd = gsub("ab", "", partybrd),
         partybrd = gsub("bis", "", partybrd),
         partybrd = trimws(partybrd),
         partybrd = str_squish(partybrd))

# emptyvalues with na
nazis.brd <- nazis.brd %>% mutate_all(~ifelse(as.character(.) == "", NA, .))

# rename columns
nazis.brd <- nazis.brd %>%
  rename("nsdap.membership" = "NSDAP",
         "position" = "Amt/Ämter",
         "wikitag" = "links")

# columns correct order, delete unnecessary 
nazis.brd <- nazis.brd %>% 
  select(name, wikitag, birthdate, deathdate, nsdap.membership, partybrd)

# save rds
saveRDS(nazis.brd, file = "nazisparl.RDS")
```

### Final Dataset

This code performs data wrangling on two datasets nazis.brd and btmember. It selects relevant columns from both datasets and performs a left join operation to combine them into a single dataset **btmembernazi**. The newly created dataset has columns from both original datasets and contains information on the party, voteshare, and NSDAP membership for all parliamentarians in the first 6 electoral periods. The final result is saved in two formats, RDS and CSV, for future use.

```{r, results='hide', error=FALSE, warning=FALSE, message=FALSE}
# datawrangling ----------------------------------------------------------------
nazis.merge <- nazis.brd %>% 
  select(wikitag, nsdap.membership)

btmember.merge <- btmember %>%
  select(name, wikitag, party, voteshare, legperiod)

btmembernazi <- left_join(btmember.merge, nazis.merge, by = "wikitag") %>%
  mutate(nsdap.membership = ifelse(is.na(nsdap.membership), 0, 1),
         party = ifelse(party == "unabhängig", "independent", party),
         voteshare = ifelse(voteshare == "", NA, voteshare))

# save rds
saveRDS(btmembernazi, file = "btmembernazi.RDS")

# save csv
write.csv(btmembernazi, file = "btmembernazi.csv")
```

